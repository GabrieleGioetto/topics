# Improving Neural Topic Models using Knowledge Distillation

Repo for our EMNLP 2020 paper. We are currently cleaning up the implementation for improved ease-of-use, but provide the code included in our original submission for the time being. Much like a dirty sock, it is functional, but has accumulated some unpleasant smells.
