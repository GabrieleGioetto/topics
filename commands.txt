python teacher/bert_reconstruction.py --input_dir ./data/imdb/processed-dev --output_dir ./data/imdb/processed-dev/logits --do_train --evaluate_during_training --logging_steps 200 --save_steps 1000 --num_train_epochs 6 --seed 42 --num_workers 4 --batch_size 20  --gradient_accumulation_steps 8 

#pycharm
python teacher/bert_reconstruction.py --input_dir ../data/imdb/processed-dev --output_dir ../data/imdb/processed-dev/logits --do_train --evaluate_during_training --logging_steps 200 --save_steps 1000 --num_train_epochs 6 --seed 42 --num_workers 4 --batch_size 20  --gradient_accumulation_steps 8 



python teacher/bert_reconstruction.py --input_dir ./data/imdb/processed-dev --output_dir ./data/imdb/processed-dev/logits --seed 42 --num_workers 6 --get_reps --checkpoint_folder_pattern "checkpoint-9000" --save_doc_logits --no_dev 

# pycharm
python teacher/bert_reconstruction.py --input_dir ../data/imdb/processed-dev --output_dir ../data/imdb/processed-dev/logits --seed 42 --num_workers 6 --get_reps --checkpoint_folder_pattern "checkpoint-9000" --save_doc_logits --no_dev 




python scholar/run_scholar.py ./data/imdb/processed-dev --dev_metric npmi -k 50 --epochs 500 --patience 500 --batch_size 200 --background_embeddings --device 0 --dev_prefix dev -l 0.002 --alpha 0.5 --eta_bn_anneal_step_const 0.25 --doc_reps_dir ./data/imdb/processed-dev/logits/checkpoint-9000/doc_logits --use_doc_layer --no_bow_reconstruction_loss --doc_reconstruction_weight 0.5 --doc_reconstruction_temp 1.0 --doc_reconstruction_logit_clipping 10.0 -o ./outputs/imdb
